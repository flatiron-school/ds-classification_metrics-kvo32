{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.datasets import load_breast_cancer, load_iris, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix,\\\n",
    "    precision_score, recall_score, accuracy_score, f1_score, log_loss,\\\n",
    "    roc_curve, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- Define cross-entropy loss\n",
    "- Define a confusion matrix\n",
    "- Describe different classification metrics such as accuracy, recall, and precision\n",
    "- Characterize AUC-ROC as a classifier metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "There are many ways to evaluate a classification model, and your choice of evaluation metric can have a major impact on how well your model serves its intended goals. This lecture will review common classification metrics you might consider using, and considerations for how to make your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario: Identifying Fraudulent Credit Card Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit card companies often try to identify whether a transaction is fraudulent at the time when it occurs, in order to decide whether to approve it. Let's build a classification model to try to classify fraudulent transactions! \n",
    "\n",
    "The data for this example from from [this Kaggle dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to downsample from original dataset\n",
    "#\n",
    "# credit_data = pd.read_csv('creditcard.csv')\n",
    "# credit_data_small = credit_data.iloc[0:10000]\n",
    "# credit_data_small.describe()\n",
    "# credit_data_small.to_csv('credit_fraud_small.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('credit_fraud_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains features for the transaction amount, the relative time of the transaction, and 28 other features formed using PCA. The target 'Class' is a 1 if the transaction was fraudulent, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Let's see what we can learn from some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5966.033400</td>\n",
       "      <td>-0.241862</td>\n",
       "      <td>0.281949</td>\n",
       "      <td>0.906270</td>\n",
       "      <td>0.264148</td>\n",
       "      <td>-0.046398</td>\n",
       "      <td>0.133108</td>\n",
       "      <td>-0.071689</td>\n",
       "      <td>-0.064778</td>\n",
       "      <td>0.802224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051990</td>\n",
       "      <td>-0.152671</td>\n",
       "      <td>-0.033268</td>\n",
       "      <td>0.021335</td>\n",
       "      <td>0.087146</td>\n",
       "      <td>0.108140</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>63.030188</td>\n",
       "      <td>0.00380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4473.403739</td>\n",
       "      <td>1.521679</td>\n",
       "      <td>1.308139</td>\n",
       "      <td>1.159154</td>\n",
       "      <td>1.441235</td>\n",
       "      <td>1.182935</td>\n",
       "      <td>1.307311</td>\n",
       "      <td>1.077430</td>\n",
       "      <td>1.259064</td>\n",
       "      <td>1.155198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913811</td>\n",
       "      <td>0.631083</td>\n",
       "      <td>0.487814</td>\n",
       "      <td>0.594430</td>\n",
       "      <td>0.428171</td>\n",
       "      <td>0.562793</td>\n",
       "      <td>0.410868</td>\n",
       "      <td>0.266247</td>\n",
       "      <td>184.486158</td>\n",
       "      <td>0.06153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-27.670569</td>\n",
       "      <td>-34.607649</td>\n",
       "      <td>-15.496222</td>\n",
       "      <td>-4.657545</td>\n",
       "      <td>-32.092129</td>\n",
       "      <td>-23.496714</td>\n",
       "      <td>-26.548144</td>\n",
       "      <td>-23.632502</td>\n",
       "      <td>-6.329801</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.468435</td>\n",
       "      <td>-8.527145</td>\n",
       "      <td>-15.144340</td>\n",
       "      <td>-2.512377</td>\n",
       "      <td>-2.577363</td>\n",
       "      <td>-1.338556</td>\n",
       "      <td>-7.976100</td>\n",
       "      <td>-3.509250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2072.750000</td>\n",
       "      <td>-1.013283</td>\n",
       "      <td>-0.208342</td>\n",
       "      <td>0.412799</td>\n",
       "      <td>-0.614424</td>\n",
       "      <td>-0.643390</td>\n",
       "      <td>-0.629934</td>\n",
       "      <td>-0.542336</td>\n",
       "      <td>-0.190747</td>\n",
       "      <td>0.070868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268120</td>\n",
       "      <td>-0.549638</td>\n",
       "      <td>-0.174120</td>\n",
       "      <td>-0.327817</td>\n",
       "      <td>-0.158137</td>\n",
       "      <td>-0.327974</td>\n",
       "      <td>-0.084489</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4563.500000</td>\n",
       "      <td>-0.372799</td>\n",
       "      <td>0.288524</td>\n",
       "      <td>0.944361</td>\n",
       "      <td>0.219861</td>\n",
       "      <td>-0.152769</td>\n",
       "      <td>-0.152566</td>\n",
       "      <td>-0.055585</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>0.805275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123273</td>\n",
       "      <td>-0.136746</td>\n",
       "      <td>-0.045794</td>\n",
       "      <td>0.079976</td>\n",
       "      <td>0.121001</td>\n",
       "      <td>0.042865</td>\n",
       "      <td>-0.004568</td>\n",
       "      <td>0.015897</td>\n",
       "      <td>15.950000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10233.250000</td>\n",
       "      <td>1.150864</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>1.602903</td>\n",
       "      <td>1.125666</td>\n",
       "      <td>0.371081</td>\n",
       "      <td>0.505357</td>\n",
       "      <td>0.476280</td>\n",
       "      <td>0.274533</td>\n",
       "      <td>1.506299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.247490</td>\n",
       "      <td>0.081665</td>\n",
       "      <td>0.410877</td>\n",
       "      <td>0.359058</td>\n",
       "      <td>0.476394</td>\n",
       "      <td>0.120811</td>\n",
       "      <td>0.077182</td>\n",
       "      <td>50.960000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15012.000000</td>\n",
       "      <td>1.960497</td>\n",
       "      <td>8.636214</td>\n",
       "      <td>4.101716</td>\n",
       "      <td>10.463020</td>\n",
       "      <td>34.099309</td>\n",
       "      <td>21.393069</td>\n",
       "      <td>34.303177</td>\n",
       "      <td>5.060381</td>\n",
       "      <td>10.392889</td>\n",
       "      <td>...</td>\n",
       "      <td>22.588989</td>\n",
       "      <td>4.534454</td>\n",
       "      <td>13.876221</td>\n",
       "      <td>3.200201</td>\n",
       "      <td>5.525093</td>\n",
       "      <td>3.517346</td>\n",
       "      <td>8.254376</td>\n",
       "      <td>4.860769</td>\n",
       "      <td>7712.430000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Time            V1            V2            V3            V4  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean    5966.033400     -0.241862      0.281949      0.906270      0.264148   \n",
       "std     4473.403739      1.521679      1.308139      1.159154      1.441235   \n",
       "min        0.000000    -27.670569    -34.607649    -15.496222     -4.657545   \n",
       "25%     2072.750000     -1.013283     -0.208342      0.412799     -0.614424   \n",
       "50%     4563.500000     -0.372799      0.288524      0.944361      0.219861   \n",
       "75%    10233.250000      1.150864      0.901879      1.602903      1.125666   \n",
       "max    15012.000000      1.960497      8.636214      4.101716     10.463020   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean      -0.046398      0.133108     -0.071689     -0.064778      0.802224   \n",
       "std        1.182935      1.307311      1.077430      1.259064      1.155198   \n",
       "min      -32.092129    -23.496714    -26.548144    -23.632502     -6.329801   \n",
       "25%       -0.643390     -0.629934     -0.542336     -0.190747      0.070868   \n",
       "50%       -0.152769     -0.152566     -0.055585      0.012865      0.805275   \n",
       "75%        0.371081      0.505357      0.476280      0.274533      1.506299   \n",
       "max       34.099309     21.393069     34.303177      5.060381     10.392889   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean   ...     -0.051990     -0.152671     -0.033268      0.021335   \n",
       "std    ...      0.913811      0.631083      0.487814      0.594430   \n",
       "min    ...    -11.468435     -8.527145    -15.144340     -2.512377   \n",
       "25%    ...     -0.268120     -0.549638     -0.174120     -0.327817   \n",
       "50%    ...     -0.123273     -0.136746     -0.045794      0.079976   \n",
       "75%    ...      0.032707      0.247490      0.081665      0.410877   \n",
       "max    ...     22.588989      4.534454     13.876221      3.200201   \n",
       "\n",
       "                V25           V26           V27           V28        Amount  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.087146      0.108140      0.005518      0.002915     63.030188   \n",
       "std        0.428171      0.562793      0.410868      0.266247    184.486158   \n",
       "min       -2.577363     -1.338556     -7.976100     -3.509250      0.000000   \n",
       "25%       -0.158137     -0.327974     -0.084489     -0.015753      5.000000   \n",
       "50%        0.121001      0.042865     -0.004568      0.015897     15.950000   \n",
       "75%        0.359058      0.476394      0.120811      0.077182     50.960000   \n",
       "max        5.525093      3.517346      8.254376      4.860769   7712.430000   \n",
       "\n",
       "             Class  \n",
       "count  10000.00000  \n",
       "mean       0.00380  \n",
       "std        0.06153  \n",
       "min        0.00000  \n",
       "25%        0.00000  \n",
       "50%        0.00000  \n",
       "75%        0.00000  \n",
       "max        1.00000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What can we learn from the mean of the target 'Class'?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "Fraudulent transactions are rare - only 0.4% of transactions were fraudulent\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Let's run a logistic regression model using all of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate data into feature and target DataFrames\n",
    "X = credit_data.drop('Class', axis = 1)\n",
    "y = credit_data['Class']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25,\n",
    "                                                   random_state=1)\n",
    "# Scale the data for modeling\n",
    "cred_scaler = StandardScaler()\n",
    "cred_scaler.fit(X_train)\n",
    "X_train_sc = cred_scaler.transform(X_train)\n",
    "X_test_sc = cred_scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regresssion model with the train data\n",
    "cred_model = LogisticRegression(random_state=42)\n",
    "cred_model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's calculate the accuracy score for our model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cred_model.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 99.88% accuracy, meaning that 99.88% of our predictions were correct! That seems great, right? Maybe... too great? Let's dig in deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Let's consider the four categories of predictions our model might have made:\n",
    "\n",
    "* Predicting that a transaction was fraudulent when it actually was (**true positive** or **TP**)\n",
    "* Predicting that a transaction was fraudulent when it actually wasn't (**false positive** or **FP**)\n",
    "* Predicting that a transaction wasn't fraudulent when it actually was (**false negative** or **FN**)\n",
    "* Predicting that a transaction wasn't fraudulent when it actually wasn't (**true negative** or **TN**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **confusion matrix** gives us all four of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2493,    0],\n",
       "       [   3,    4]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cred_model.predict(X_test_sc)\n",
    "cm_1 = confusion_matrix(y_test, y_pred)\n",
    "cm_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the way that sklearn displays its confusion matrix: The rows are \\['actually false', 'actually true'\\]; the columns are \\['predicted false', 'predicted true'\\].\n",
    "\n",
    "So it displays:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "TN & FP \\\\\n",
    "FN & TP\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do you see anything surprising in the confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "Let's calculate some common classification metrics and consider which would be most useful for this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = cm_1[0, 0]\n",
    "fp = cm_1[0, 1]\n",
    "fn = cm_1[1, 0]\n",
    "tp = cm_1[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "**Accuracy** = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "In words: How often did my model correctly identify transactions (fraudulent or not fraudulent)? This should give us the same value as we got from the `.score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988\n"
     ]
    }
   ],
   "source": [
    "acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "**Recall** = **Sensitivity** = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "In words: How many of the actually fraudulent transactions did my model identify? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "rec = tp / (tp + fn)\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do you think a credit card company would consider recall to be an important metric? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "**Precision** = **Specificity** = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "In words: How often was my model's prediction of 'fraudulent' correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "prec = tp / (tp + fp)\n",
    "print(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do you think a credit card company would care more about recall or precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F Score\n",
    "\n",
    "The F score is a combination of precision and recall, which can be useful when both are important for a business problem. Most common is the **F-1 Score**, which is an equal balance of the two using a [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean).\n",
    "\n",
    "**F-1 Score** = $\\frac{2PrRc}{Pr + Rc}$ = $\\frac{2TP}{2TP + FP + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "f1_score = 2*prec*rec / (prec + rec)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which of these metrics do you think a credit card company would care most about when trying to flag fraudulent transactions to deny?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `classification_report()`\n",
    "\n",
    "You can get all of these metrics using the `classification_report()` function. \n",
    "\n",
    "- The top rows show statistics for if you treated each label as the \"positive\" class\n",
    "- **Support** shows the sample size in each class\n",
    "- The averages in the bottom two rows are across the rows in the class table above (useful when there are more than two classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2493\n",
      "           1       1.00      0.57      0.73         7\n",
      "\n",
      "    accuracy                           1.00      2500\n",
      "   macro avg       1.00      0.79      0.86      2500\n",
      "weighted avg       1.00      1.00      1.00      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario: Breast Cancer Prediction\n",
    "\n",
    "Let's evaulate a model using scikit-learn's breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, random_state=42)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "preds, target = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(preds, target,\n",
    "                                                   random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "bc_scaler = StandardScaler()\n",
    "bc_scaler.fit(X_train)\n",
    "X_train_sc = bc_scaler.transform(X_train)\n",
    "X_test_sc = bc_scaler.transform(X_test)\n",
    "\n",
    "# Run the model\n",
    "bc_model = LogisticRegression(solver='lbfgs', max_iter=10000,\n",
    "                           random_state=42)\n",
    "bc_model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Calculate the following for this model:\n",
    "\n",
    "- Confusion Matrix\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "\n",
    "Discuss: Which one would you choose to evaluate the model for use as a diagnostic tool to detect breast cancer? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53  1]\n",
      " [ 2 87]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        54\n",
      "           1       0.99      0.98      0.98        89\n",
      "\n",
      "    accuracy                           0.98       143\n",
      "   macro avg       0.98      0.98      0.98       143\n",
      "weighted avg       0.98      0.98      0.98       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "y_pred = bc_model.predict(X_test_sc)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Which Metric Should I Care About?\n",
    "\n",
    "Well, it depends.\n",
    "\n",
    "Accuracy:\n",
    "- Pro: Takes into account both false positives and false negatives.\n",
    "- Con: Can be misleadingly high when there is a significant class imbalance. (A lottery-ticket predictor that *always* predicts a loser will be highly accurate.)\n",
    "\n",
    "Recall:\n",
    "- Pro: Highly sensitive to false negatives.\n",
    "- Con: No sensitivity to false positives.\n",
    "\n",
    "Precision:\n",
    "- Pro: Highly sensitive to false positives.\n",
    "- Con: No sensitivity to false negatives.\n",
    "\n",
    "F-1 Score:\n",
    "- Harmonic mean of recall and precision.\n",
    "\n",
    "The nature of your business problem will help you determine which metric matters.\n",
    "\n",
    "Sometimes false positives are much worse than false negatives: Arguably, a model that compares a sample of crime-scene DNA with the DNA in a city's database of its citizens presents one such case. Here a false positive would mean falsely identifying someone as having been present at a crime scene, whereas a false negative would mean only that we fail to identify someone who really was present at the crime scene as such.\n",
    "\n",
    "On the other hand, consider a model that inputs X-ray images and predicts the presence of cancer. Here false negatives are surely worse than false positives: A false positive means only that someone without cancer is misdiagnosed as having it, while a false negative means that someone with cancer is misdiagnosed as *not* having it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Cost Matrix\n",
    "\n",
    "One might assign different weights to the costs associated with false positives and false negatives. (We'll standardly assume that the costs associated with *true* positives and negatives are negligible.)\n",
    "\n",
    "**Example**. Suppose we are in the DNA prediction scenario above. Then we might construct the following cost matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 10],\n",
       "       [ 3,  0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = np.array([[0, 10], [3, 0]])\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cost matrix will allow us to compare models if we have access to those models' rates of false positives and false negatives, i.e. if we have access to the models' confusion matrices!\n",
    "\n",
    "**Problem**. Given the cost matrix above and the confusion matrices below, which model should we go with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100  10]\n",
      " [ 30 300]] \n",
      "\n",
      " [[120  20]\n",
      " [  0 300]]\n"
     ]
    }
   ],
   "source": [
    "conf1, conf2 = np.array([[100, 10], [30, 300]]), np.array([[120, 20], [0, 300]])\n",
    "\n",
    "print(conf1, 2*'\\n', conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Multiple Classes\n",
    "\n",
    "We can understand these metrics of recall, precision, and the rest even if there are more than two classes in our classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(flowers.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_train, dims_test, spec_train, spec_test = train_test_split(flowers.data,\n",
    "                                                                flowers.target,\n",
    "                                                                test_size=0.5,\n",
    "                                                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_f = StandardScaler()\n",
    "\n",
    "ss_f.fit(dims_train)\n",
    "\n",
    "dims_train_sc = ss_f.transform(dims_train)\n",
    "dims_test_sc = ss_f.transform(dims_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, multi_class='multinomial', random_state=42)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_f = LogisticRegression(multi_class='multinomial',\n",
    "                             C=0.01, random_state=42)\n",
    "\n",
    "logreg_f.fit(dims_train_sc, spec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(estimator=logreg_f,\n",
    "                      X=dims_test_sc,\n",
    "                      y_true=spec_test,\n",
    "                     display_labels=[\n",
    "                         'setosa',\n",
    "                         'versicolor',\n",
    "                         'virginica'\n",
    "                            ]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       0.94      0.65      0.77        23\n",
      "           2       0.73      0.96      0.83        23\n",
      "\n",
      "    accuracy                           0.88        75\n",
      "   macro avg       0.89      0.87      0.87        75\n",
      "weighted avg       0.90      0.88      0.88        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(spec_test,\n",
    "              logreg_f.predict(dims_test_sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these metrics, we choose one class to be the \"positive\" class, and the rest are assigned to the \"negative\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAK Lecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data = pd.read_csv('credit_fraud_small.csv')\n",
    "\n",
    "# Separate data into feature and target DataFrames\n",
    "X = credit_data.drop('Class', axis = 1)\n",
    "y = credit_data['Class']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25,\n",
    "                                                   random_state=1)\n",
    "# Scale the data for modeling\n",
    "cred_scaler = StandardScaler()\n",
    "cred_scaler.fit(X_train)\n",
    "X_train_sc = cred_scaler.transform(X_train)\n",
    "X_test_sc = cred_scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regresssion model with the train data\n",
    "cred_model = LogisticRegression(random_state=42)\n",
    "cred_model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "***`.predict()` vs. `.predict_proba()`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99949036e-01, 5.09641701e-05],\n",
       "       [9.99976279e-01, 2.37213738e-05],\n",
       "       [9.99998007e-01, 1.99332418e-06],\n",
       "       [9.99994131e-01, 5.86929843e-06],\n",
       "       [9.99991505e-01, 8.49504830e-06]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model.predict_proba(X_test_sc)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies\n",
    "\n",
    "There are a couple of strategies for trying to quantify these important differences in value between false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust the thresholds for false positives and false negatives.\n",
    "\n",
    "Let's turn to this now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Area Under the Curve of the Receiver Operating Characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "First let's generate some data using sklearn's make_classification tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=10000, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now we'll fit a LogisticRegression object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "logreg.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, logreg.predict(X_test_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining true/false positives/negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = cm[1][1], cm[0][0], cm[0][1], cm[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC-ROC\n",
    "\n",
    "The Receiver Operating Characteristic curve plots the true-positive rate vs. the false-positive rate. Let's define these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholds\n",
    "\n",
    "Wait. How does this make sense? Doesn't a classifier just have a certain number of true positives, false positives, and all the rest? And so wouldn't a \"plot\" of these rates just be a single point on a graph?\n",
    "\n",
    "Consider a prediction for a particular data point. The features have particular values that lead the model to predict 0 or 1, one way or the other. But the model doesn't merely spit out 0's and 1's: As we just saw, there is a *calculation* done here. Let's look again at the predicted probabilities of class membership for a particular point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(X_test_sc)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict(X_test_sc)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the default behavior is simply to take the larger of these values as the \"real\" prediction. Since $0.84 > 0.16$, we'll understand the model to be predicting this point to belong to class \"1\" (or the positive class). An equivalent way of understanding the default behavior is that we:\n",
    "\n",
    "- round the predicted numbers up to 1 if they are at least as large as 0.5; and\n",
    "- round them down to 0 if they are less than 0.5.\n",
    "\n",
    "Since the probabilities must sum to 1, there will never be any problem with this algorithm.\n",
    "\n",
    "But we don't have to do things this way. Suppose we're building a model that predicts the presence of prostate cancer from X-ray scans of prostates. And suppose we get a pair of probabilities for some particular scan that look like this:\n",
    "\n",
    "- pred_neg: 0.52, pred_pos: 0.48\n",
    "\n",
    "Because false negatives (cancerous prostates mislabeled) are *much* more costly than false positives (non-cancerous prostates mislabeled), we may well want to **adjust our threshold** of classification. We might want to have our model predict \"positive\" if the corresponding probability is, say, as low as 0.4, or maybe even as low as 0.1. (Speaking for myself, if there was even a 10% chance that my prostate was cancerous, I think I'd probably want to know about it.)\n",
    "\n",
    "Clearly, the true- and false-positive rates will change if we make this adjustment to the threshold. In fact, in the present case that was the whole point of making the adjustment: We want to minimize our false negatives.\n",
    "\n",
    "So this is how the plot of these rates takes shape.\n",
    "\n",
    "Let's build a function that will take in our data, together with a threshold setting, and return the corresponding true- and false-positive rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_rates(X_train, X_test, y_train, y_test, thresh):\n",
    "    logreg = LogisticRegression(solver='liblinear')\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_hat_probs = logreg.predict_proba(X_test)\n",
    "    y_hat = []\n",
    "    for val in y_hat_probs:                                 # Each element in y_hat_probs is an array.\n",
    "        if val[0] < thresh:                                 # We'll set our own threshold for classifying\n",
    "            y_hat.append(1)                                 # a test point as positive! The lower my threshold,\n",
    "        else:                                               # the fewer predicted positives I'll have. For the\n",
    "            y_hat.append(0)                                 # cancer example, I'd want to set a *high* threshold.\n",
    "    cm = confusion_matrix(y_test, y_hat)\n",
    "    tp, tn, fp, fn = cm[1][1], cm[0][0], cm[0][1], cm[1][0]\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    return tpr, fpr, f'tpr:{round(tpr, 3)}, fpr:{round(fpr, 3)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True- and false-positive rates for various thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in np.linspace(0, 1, 11):\n",
    "    print(f'Rates at threshold = {round(x, 1)}: ' + classify_rates(X_train_sc, X_test_sc, y_train, y_test, x)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my threshold goes up, I'll have more positive predictions, which means I'll have both more true positives and more false positives.\n",
    "\n",
    "Note:\n",
    "\n",
    "- I can artificially increase my true-positive rate to 1 by setting my threshold to 1, but at that point my false-positive rate is also 1! I'll have no true negatives and no false negatives. This will arise naturally if my training data has **very few (actual) negatives**. This was the problem in Lottery Scenario 1.\n",
    "- I can artificially reduce my false-positive rate to 0 by setting my threshold to 0, but at  that point my true-positive rate is also 0! I'll have no true positives and no false positives. This will arise naturally if my training data has **very few (actual)  positives**. This was the problem in Lottery Scenario 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Under the Curve\n",
    "\n",
    "The ROC curve will be a plot of tpr (on the y-axis) vs. fpr (on the x-axis). There will always be a point at (0, 0) and another at (1, 1). The question is what happens in the middle. Since we want our y-values to be as high as possible for any particular x-value, a natural metric is to calculate the **area under the curve**. The larger the area, the better the classifier. The maximum possible area is the area of the whole box between 0 and 1 on both axes, so that's a **maximum area of 1**.\n",
    "\n",
    "What's the minimum? Well that depends on the ratios of (actual) positive and negatives in my data, in much the way that a baseline accuracy score does.\n",
    "\n",
    "Remember: If my test data comprises 90% positives and only 10% negatives, then a simple classifier that always predicts \"positive\" will be 90% accurate! And so that would be the baseline level for a classifier on that data.\n",
    "\n",
    "If we have equal numbers of positives and negatives, then we can set an **abolute minimum area of 0.5**. That's the \"curve\" we'd get by plotting a straight diagonal line from (0, 0) to (1, 1).\n",
    "\n",
    "Why? The area under the curve really represents the test's ability to **discriminate** positives from negatives. Suppose I randomly took several pairs of points, one positive and one negative, and checked my test's predictions. The area under the curve represents a threshold-independent measure of how often my test would get the two predictions correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Curve\n",
    "\n",
    "Let's plot our own ROC curve. We'll create an array of different thresholds and use our `classify_rates()` function to get the true- and false-positive rates for each threshold.\n",
    "\n",
    "One way of choosing a threshold **independently of business concerns** is to select the point on the curve that is furthest from (1, 0), the \"worse-case\" point where our true-positive rate is 0 and our false-positive rate is 1. So let's find that point as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tprs = []\n",
    "fprs = []\n",
    "diffs = []\n",
    "for x in np.linspace(0, 1, 101):\n",
    "    fprs.append(classify_rates(X_train_sc, X_test_sc, y_train, y_test, x)[1])\n",
    "    tprs.append(classify_rates(X_train_sc, X_test_sc, y_train, y_test, x)[0])\n",
    "    diffs.append(np.sqrt(tprs[-1]**2 + (1-fprs[-1])**2))\n",
    "    \n",
    "max_dist = diffs.index(np.max(diffs))\n",
    "print(f\"\"\"With a threshold of {(max_dist - 1) / 100}: \\n\"\"\"\n",
    "      f\"\"\"\\tYou\\'ll have a True Positive Rate of {round(tprs[max_dist], 3)} \\n\"\"\"\n",
    "      f\"\"\"\\tand a False Positive Rate of {round(fprs[max_dist], 3)}\"\"\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(fprs[:max_dist], tprs[:max_dist], 'r.')\n",
    "ax.plot(fprs[max_dist], tprs[max_dist], 'ko', ms=10)\n",
    "ax.plot(fprs[max_dist + 1:], tprs[max_dist + 1:], 'r.')\n",
    "ax.plot(fprs, fprs, '.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn's `roc_auc_score()` function will compute the area under the curve for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(roc_auc_score(y_test, logreg.predict(X_test_sc)), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our curve with scikit-learn's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict(X_test_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn only shows us the optimal threshold, but it appears to be very similar to ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling\n",
    "\n",
    "One of the most effective strategies to handle these cases is **oversampling the minority class**. That is, I give myself more data points than I really have. I could achieve this either by [bootstrapping](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html) or by generating some data that is fake but close to actual data. The latter is the idea behind [SMOTE](https://imbalanced-learn.org/stable/over_sampling.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Loss Functions\n",
    "\n",
    "Another more \"natural\" way of measuring the quality of a classifier is just to look at the loss function, which will often be the **log loss**. In cases with multiple classes, we use **cross-entropy loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.95005909317609"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Loss by Hand\n",
    "\n",
    "Log loss is generally calculated as an average per data point, and is computed as follows:\n",
    "\n",
    "$L(y, \\hat{y}) = -\\frac{1}{N}\\sum^N_{i=1}[y_i\\ln(\\hat{y_i}) + (1-y_i)\\ln(1-\\hat{y_i})]$,\n",
    "\n",
    "where $y$ is the vector of true values and $\\hat{y}$ is the vector of probabilities that the point in question has a correct label of 1.\n",
    "\n",
    "- Suppose, for a given data point, that the correct prediction of the label is **0**. In that case, the contribution from that point to the sum in the loss function defined above will be $-\\ln(1-\\hat{y_i})$. So, the closer the prediction for that point is to 0, the closer the contribution to the sum will be to $-\\ln(1)=0$. But as the prediction gets closer to 1, the closer the contribution will be to $-\\ln(0)=\\infty$.\n",
    "\n",
    "- Suppose, on the other hand, that the correct prediction is **1**. In that case, the contribution from that point to the sum in the loss function defined above will be $-\\ln(\\hat{y_i})$. So, the closer the prediction for that point is to 1, the closer the contribution to the sum will be to $-\\ln(1)=0$. But as the prediction gets closer to 0, the closer the contribution will be to $-\\ln(0)=\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = list(zip(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (0, 0), (0, 1), (0, 1), (0, 0)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-63b5355f8e8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myi_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myi_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi_hat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcalc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-63b5355f8e8e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myi_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myi_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi_hat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcalc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "calc = [-(yi * np.log(yi_hat[1]) + (1 - yi) * np.log(yi_hat[0])) for (yi, yi_hat) in compare]\n",
    "calc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3322228531070542"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(calc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
